{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    spam_paths = np.array([])\n",
    "    ham_paths = np.array([])\n",
    "    for subdir, dirs, files in os.walk('data'):\n",
    "        for file in files:\n",
    "            if file != 'Summary.txt' and file[-4:] == '.txt':\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file_path[-8:] == 'spam.txt':\n",
    "#                     spam_paths.append(file_path)\n",
    "                    spam_paths = np.append(spam_paths, file_path)\n",
    "                elif file_path[-7:] == 'ham.txt':\n",
    "#                     ham_paths.append(file_path)\n",
    "                    ham_paths = np.append(ham_paths, file_path)\n",
    "                    \n",
    "    dataset = [(path,1) for path in spam_paths] + [(path,0) for path in ham_paths]\n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    validation_ratio = 0.1\n",
    "    num_train = int(train_ratio * len(dataset))\n",
    "    num_validation = int(validation_ratio * len(dataset))\n",
    "    \n",
    "    train_set = dataset[:num_train]\n",
    "    validation_set = dataset[num_train : num_train + num_validation]\n",
    "    test_set = dataset[num_train + num_validation:]\n",
    "    \n",
    "    train_filePaths_X, train_y = zip(*train_set)\n",
    "    validation_filePaths_X, validation_y = zip(*validation_set)\n",
    "    test_filePaths_X, test_y = zip(*test_set)\n",
    "    \n",
    "    \n",
    "    print(f'Number of files: {len(spam_paths)+len(ham_paths)}')\n",
    "    print(f'Number of spam: {len(spam_paths)}')\n",
    "    print(f'Number of ham: {len(ham_paths)}')\n",
    "    print(f\"Size of train_set: {len(train_set)}\")\n",
    "    print(f\"Size of validation_set: {len(validation_set)}\")\n",
    "    print(f\"Size of test_set: {len(test_set)}\")\n",
    "    \n",
    "    return train_filePaths_X, train_y, validation_filePaths_X, validation_y, test_filePaths_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_dataset(dataset_X):\n",
    "    \n",
    "    for filePath in dataset_X:\n",
    "        file = open(filePath, 'r', errors='ignore')\n",
    "        yield file\n",
    "        file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfs(file):\n",
    "    result = {}\n",
    "    num_words = 0\n",
    "#     print(file)\n",
    "    for line in file:\n",
    "#         print(line)\n",
    "        words = tokenizer.tokenize(line)\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        for word in words:\n",
    "            if word in result.keys():\n",
    "                result[word] += 1\n",
    "            else:\n",
    "                result[word] = 1\n",
    "                \n",
    "            num_words += 1\n",
    "            \n",
    "    for key in result:\n",
    "        result[key] = result[key] / num_words\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_largest_tfidfs(d, num_features):\n",
    "    \"\"\"\n",
    "    Return num_features words with largest tf_idfs in dictionary d\n",
    "    \n",
    "    #HAVE TO DECIDE WHAT TO DO IF LESS THAN NUM_FEATURES FEATURES PRESENT\n",
    "    \n",
    "    @param d: the dictionary containing tf-idfs\n",
    "    @param num_features: integer>0 of the number of features to get\n",
    "    \"\"\"\n",
    "    result = {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}\n",
    "    return list(result.keys())[-num_features:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idfs(corpus):\n",
    "    \"\"\"\n",
    "    Calculates the idf for every term in the document\n",
    "    \"\"\"\n",
    "    DF = {} #dictionary --> {word: set() of documents in which word present} key:word, value: set\n",
    "    num_documents = len(corpus)\n",
    "    for file in iter_dataset(corpus):\n",
    "#         print(file)\n",
    "        for line in file:\n",
    "#             print(line)\n",
    "            words = tokenizer.tokenize(line)\n",
    "            words = [stemmer.stem(word) for word in words]\n",
    "            for word in words:\n",
    "                try:\n",
    "                    DF[word].add(file)\n",
    "                except:\n",
    "                    DF[word] = {file}\n",
    "    \n",
    "    for key in DF.keys():\n",
    "#         print(len(DF[key]))\n",
    "        DF[key] = np.log(num_documents/(len(DF[key])+1))\n",
    "    \n",
    "    return DF\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_matrix(corpus, idfs, num_features):\n",
    "    \"\"\"\n",
    "    calculates the tf_idf for each word for each file in the corpus\n",
    "    \n",
    "    Returns the feature matrix - top num_features words by tf-idf for each file - [num_features * len(corpus)] numpy array\n",
    "    \n",
    "    \n",
    "    @param corpus: np Array [1*m] of filePaths\n",
    "    @param idfs: the idfs dictionary for all words in the corpus\n",
    "    @param num_features: the number of t to extract\n",
    "    \"\"\"\n",
    "    #using for loop instead of vectorization because limit on how many files can be opened\n",
    "    feature_matrix = np.array([])\n",
    "    for file in iter_dataset(corpus):\n",
    "        tfs = get_tfs(file)\n",
    "        tfidfs = {}\n",
    "    \n",
    "        for key in tfs:\n",
    "#             print(tfs[key])\n",
    "            tfidfs[key] = tfs[key] * idfs[key]\n",
    "            \n",
    "        features_file = get_words_with_largest_tfidfs(tfidfs, num_features)\n",
    "        \n",
    "        feature_matrix = np.append(feature_matrix, np.transpose(features_file))\n",
    "\n",
    "    return tfidfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 33716\n",
      "Number of spam: 17171\n",
      "Number of ham: 16545\n",
      "Size of train_set: 26972\n",
      "Size of validation_set: 3371\n",
      "Size of test_set: 3373\n"
     ]
    }
   ],
   "source": [
    "train_X_filePaths, train_y, validation_X_filePaths, validation_y, test_X_filePaths, test_y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = get_idfs(train_X_filePaths+validation_X_filePaths+test_X_filePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 20\n",
    "train_X = get_feature_matrix(train_X_filePaths, idfs, num_features)\n",
    "validation_X = get_feature_matrix(validation_X_filePaths, idfs, num_features)\n",
    "test_X = get_feature_matrix(test_X_filePaths, idfs, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 0.0,\n",
       " 'on': 0.005686066452973173,\n",
       " 'the': 0.006144005175132828,\n",
       " 'front': 0.016173318935663164,\n",
       " 'page': 0.028456815709052567,\n",
       " 'of': 0.006267230828937276,\n",
       " 'googl': 0.10508519865386344,\n",
       " 'for': 0.005199025062960624,\n",
       " 'free': 0.009280013507806936,\n",
       " 'hi': 0.008441180895267367,\n",
       " 'john': 0.012060488893973235,\n",
       " 'i': 0.0038009287631351504,\n",
       " 'hope': 0.011354177068828557,\n",
       " 'all': 0.00496093270475108,\n",
       " 'is': 0.011875026931017468,\n",
       " 'well': 0.009745191462983065,\n",
       " 'weiguarante': 0.0403841518695329,\n",
       " 'to': 0.0077084564880209394,\n",
       " 'putiyour': 0.0403841518695329,\n",
       " 'webisiteon': 0.0403841518695329,\n",
       " 'firstipag': 0.11252406746884833,\n",
       " 'ofi': 0.0403841518695329,\n",
       " 'and': 0.010135952402010127,\n",
       " 'we': 0.02332210295052366,\n",
       " 'willido': 0.0403841518695329,\n",
       " 'it': 0.007336691283563053,\n",
       " 'forifre': 0.0403841518695329,\n",
       " 'whi': 0.013321526150984543,\n",
       " 'weibeliev': 0.0403841518695329,\n",
       " 'that': 0.0037162881461659646,\n",
       " 'you': 0.013177837613375987,\n",
       " 'should': 0.007990376549942347,\n",
       " 'see': 0.007683718904076874,\n",
       " 'theiresult': 0.0807683037390658,\n",
       " 'befor': 0.009810128274879861,\n",
       " 'bookitheiservic': 0.0403841518695329,\n",
       " 'manyicompani': 0.0403841518695329,\n",
       " 'willinot': 0.0403841518695329,\n",
       " 'do': 0.005807763025928155,\n",
       " 'thi': 0.014760696300663768,\n",
       " 'becaus': 0.010634214715667561,\n",
       " 'theyicannot': 0.0403841518695329,\n",
       " 'produc': 0.013854729653970105,\n",
       " 'can': 0.005153264499480194,\n",
       " 'williprov': 0.0403841518695329,\n",
       " 'oriobligationito': 0.0403841518695329,\n",
       " 'want': 0.007633063839155238,\n",
       " 'tryiit': 0.0403841518695329,\n",
       " 'call': 0.00833801040784687,\n",
       " 'our': 0.004646491864097345,\n",
       " 'trialihotlin': 0.0403841518695329,\n",
       " 'freephon': 0.03294946527522561,\n",
       " '0800': 0.022755540906672485,\n",
       " '011': 0.05015519623604557,\n",
       " '2047': 0.06926378621939865,\n",
       " 'or': 0.004528636636349373,\n",
       " 'intern': 0.011359878069395672,\n",
       " '0044': 0.03202355842329943,\n",
       " '800': 0.01611309469133016,\n",
       " 'leav': 0.014771106664871098,\n",
       " 'yourinam': 0.0403841518695329,\n",
       " 'emailiand': 0.0403841518695329,\n",
       " 'webi': 0.03230983589834902,\n",
       " 'site': 0.019581080606731194,\n",
       " 'address': 0.009666056055881065,\n",
       " 'thenw': 0.0403841518695329,\n",
       " 'willbegin': 0.0403841518695329,\n",
       " 'process': 0.010034691755742915,\n",
       " 'put': 0.0121885239251367,\n",
       " 'your': 0.018925648360287425,\n",
       " 'web': 0.01211715977751016,\n",
       " 'siteon': 0.0403841518695329,\n",
       " 'thefirstipageof': 0.0403841518695329,\n",
       " 'what': 0.007715991566305035,\n",
       " 'will': 0.011988466562579257,\n",
       " 'happenithen': 0.0403841518695329,\n",
       " 'onc': 0.01220947454374906,\n",
       " 'have': 0.009236694553018886,\n",
       " 'got': 0.013454838503626937,\n",
       " 'onto': 0.019325339358189215,\n",
       " 'send': 0.009101881928354493,\n",
       " 'an': 0.005770018576906826,\n",
       " 'emailito': 0.0403841518695329,\n",
       " 'confirm': 0.012074019485071565,\n",
       " 'then': 0.010313278074571249,\n",
       " 'go': 0.007820821838522072,\n",
       " 'com': 0.005271251939725932,\n",
       " 'type': 0.02599195070967581,\n",
       " 'ina': 0.029011371687270648,\n",
       " 'phraseithat': 0.0403841518695329,\n",
       " 'someon': 0.014956930742161442,\n",
       " 'would': 0.006529605882324662,\n",
       " 'use': 0.007277704860890857,\n",
       " 'look': 0.007789335020218767,\n",
       " 'webisit': 0.10389567932909798,\n",
       " 'watch': 0.012920504635021611,\n",
       " 'come': 0.00994178116463299,\n",
       " 'up': 0.006725321653524119,\n",
       " 'again': 0.011413468561830037,\n",
       " 'andiagain': 0.0403841518695329,\n",
       " 'theifirst': 0.0403841518695329,\n",
       " 'youiwil': 0.0403841518695329,\n",
       " 'beiamaz': 0.0403841518695329,\n",
       " 'with': 0.010003430775640252,\n",
       " 'a': 0.00360584327200961,\n",
       " 'list': 0.008861768308767155,\n",
       " 'thinkiof': 0.0403841518695329,\n",
       " 'how': 0.01843880837289207,\n",
       " 'manyihit': 0.0403841518695329,\n",
       " 'wouldirec': 0.0403841518695329,\n",
       " 'mani': 0.010088932758097077,\n",
       " 'newtclient': 0.0403841518695329,\n",
       " 'couldtobtain': 0.0403841518695329,\n",
       " 'thetfe': 0.0403841518695329,\n",
       " 'ourtservic': 0.0807683037390658,\n",
       " 'istfantasticivalu': 0.0403841518695329,\n",
       " 'onlyi': 0.037508022489616114,\n",
       " '99': 0.013858200484447143,\n",
       " '95': 0.014823355551582136,\n",
       " 'tfor': 0.0403841518695329,\n",
       " 'onetyear': 0.0403841518695329,\n",
       " 'isitruli': 0.0403841518695329,\n",
       " 'uniqueiand': 0.0403841518695329,\n",
       " 'onli': 0.007455661162202189,\n",
       " 'soimani': 0.0403841518695329,\n",
       " 'topispot': 0.0403841518695329,\n",
       " 'canioff': 0.0403841518695329,\n",
       " 'so': 0.007518475069937308,\n",
       " 'dontidelay': 0.0403841518695329,\n",
       " 'tryiour': 0.0403841518695329,\n",
       " 'servicetfortfre': 0.0403841518695329,\n",
       " 'noioblig': 0.0403841518695329,\n",
       " 'iguaranteedt': 0.0403841518695329,\n",
       " 'there': 0.007547206297803319,\n",
       " 'isinoth': 0.0403841518695329,\n",
       " 'toilos': 0.0403841518695329,\n",
       " 'tele': 0.02958460960627819,\n",
       " 'line': 0.011004702546710286,\n",
       " 'telecom': 0.024364526512517973,\n",
       " 'leaderiof': 0.0403841518695329,\n",
       " 'webtsitetpromot': 0.0403841518695329,\n",
       " 'businessito': 0.035825594655142394,\n",
       " 'businessiemail': 0.037508022489616114,\n",
       " 'in': 0.004935744061580264,\n",
       " 'fulliaccord': 0.037508022489616114,\n",
       " 'uktlaw': 0.0403841518695329,\n",
       " 'yourtemail': 0.0403841518695329,\n",
       " 'addressiha': 0.0403841518695329,\n",
       " 'not': 0.0133981912159941,\n",
       " 'been': 0.006993323810734132,\n",
       " 'harvestedibi': 0.0403841518695329,\n",
       " 'ani': 0.011098166638559606,\n",
       " 'automaticisoftwar': 0.0403841518695329,\n",
       " 'aionettim': 0.0403841518695329,\n",
       " 'emailts': 0.0403841518695329,\n",
       " 'directli': 0.014970527840926236,\n",
       " 'emailiaddress': 0.0403841518695329,\n",
       " 'part': 0.011029213709255041,\n",
       " 'anyidatabasetand': 0.0403841518695329,\n",
       " 'be': 0.0032420532953573162,\n",
       " 'passedtonto': 0.0403841518695329,\n",
       " 'otheriparti': 0.0403841518695329,\n",
       " 'ifiy': 0.037508022489616114,\n",
       " 'receivedithisiemail': 0.0403841518695329,\n",
       " 'errortpleas': 0.037508022489616114,\n",
       " 'accept': 0.012516976502893132,\n",
       " 'ouriapolog': 0.037508022489616114,\n",
       " 'regard': 0.00774692465922256,\n",
       " 'ted': 0.0199996917627443}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X_filepaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-94ce6c017c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_idfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X_filepaths\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvalidation_X_filepaths\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_X_filepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X_filepaths' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\" Returns sigmoid of z \"\"\"\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\" Initialize w array with dim*1 zeros and b as 0\"\"\"\n",
    "    w = np.zeros((dim))\n",
    "    b = 0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, y):\n",
    "    \"\"\"\n",
    "    Apply forward and backward propagation.\n",
    "    \n",
    "    @param w: weights \n",
    "    @precond: np array shape n*1\n",
    "    \n",
    "    @param b: bias \n",
    "    @precond: float\n",
    "    \n",
    "    @param X: the features \n",
    "    @precond: np array shape n*m n is the number of features and m the number of training examples\n",
    "    \n",
    "    @param y: weights \n",
    "    @precond: np array shape 1*m the true values corresponding to X i.e. X(:, i) --> y(i)\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    #Forward prop\n",
    "    z = np.dot(np.transpose(w), X) + b # z: [1*m] matrix\n",
    "    y_hat = sigmoid(z) #y_hat: [1*m] matrix\n",
    "    cost = (-1/m) * np.sum(y * np.log(y_hat) + (1-y) * np.log(1-y_hat)) #number\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    #Backward prop\n",
    "    dw = (1/m) * np.dot(X,np.transpose(y_hat - y))\n",
    "    db = (1/m) * np.sum(y_hat - y)\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, y, epochs, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    Optimizes w, b using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    @param w: weights\n",
    "    @precond: np array shape n * 1\n",
    "\n",
    "    @param b: bias \n",
    "    @precond: -- float\n",
    "    \n",
    "    @param X - the features\n",
    "    @precond: np array shape (n, m)\n",
    "    \n",
    "    @param y - the true labels\n",
    "    @precond: np array (1, m) containing 1 if spam, 0 if ham.\n",
    "\n",
    "    @param epochs\n",
    "    @precond: integer >= 1\n",
    "    \n",
    "    @param learning_rate - lr for gradient descent\n",
    "    @precond: integer > 0\n",
    "    \n",
    "    Return:\n",
    "    params: dictionary containing the weights w and bias b\n",
    "    grads: dictionary containing dw and db (the gradients)\n",
    "    costs: list of costs produced every 100 iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        grads, cost = propagate(w, b, X, y)\n",
    "\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.19033591]\n",
      " [0.12259159]]\n",
      "b = 1.9253598300845747\n",
      "dw = [[0.67752042]\n",
      " [1.41625495]]\n",
      "db = 0.21919450454067657\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, epochs= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    @param w: weights\n",
    "    @precond: np array shape n * 1\n",
    "\n",
    "    @param b: bias \n",
    "    @precond: -- float\n",
    "    \n",
    "    @param X - the features\n",
    "    @precond: np array shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    y_preds -- a numpy array (vector) containing predictions (0/1)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    \n",
    "    y_hat = sigmoid(np.dot(np.transpose(w), X) + b)\n",
    "    y_preds = (y_hat>0.5).astype(int)\n",
    "    \n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_X, train_Y, validation_X, validation_y, test_X, test_Y,\\\n",
    "          epochs = 2000, learning_rate = 0.5, print_cost=True):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    @params train_X, train_Y, validation_X, validation_Y, test_X, test_Y: training X & Y, test X & Y\n",
    "    @preconds: represented by a numpy array of shape (n, m_train), (1, m_train), (n, m_val), (1, m_val), \n",
    "    (n, m_test) and (1,m_test) respectively\n",
    "    \n",
    "\n",
    "    @param epochs: number of iterations to optimize the parameters\n",
    "    @precond: integer > 0\n",
    "    \n",
    "    @param learning_rate -- the learning rate used in the update rule of optimize()\n",
    "    @precond: float > 0\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    m_train = train_X.shape[1]\n",
    "    print(m_train)\n",
    "    w, b = initialize_with_zeros(m_train)\n",
    "\n",
    "    parameters, grads, costs = optimize(w, b, X, y, epochs, learning_rate)\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "        \n",
    "    train_preds = predict(w, b, X_train)\n",
    "    validation_preds = predict(w, b, X_validation)\n",
    "    test_preds = predict(w, b, X_test)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(train_preds - train_y)) * 100))\n",
    "    print(\"validation accuracy: {} %\".format(100 - np.mean(np.abs(validation_preds - validation_y)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(test_preds - test_y)) * 100))\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-06513ae86d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-138-186dc5432fd7>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(train_X, train_Y, validation_X, validation_y, test_X, test_Y, epochs, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mm_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_with_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "d = model(train_X, train_y, validation_X, validation_y, test_X, test_y, epochs = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(idfs):\n",
    "    for file in iter_dataset(train_X[:3]):\n",
    "        tfidfs = get_tfidf(file, idfs)\n",
    "        top20Words = [*tfidfs.keys()][-20:]\n",
    "#         print(dict_stats(top20Words))\n",
    "        print(top20Words)\n",
    "        input()\n",
    "        #pass it to the logistic regression unit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
