{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    spam_paths = np.array([])\n",
    "    ham_paths = np.array([])\n",
    "    for subdir, dirs, files in os.walk('data'):\n",
    "        for file in files:\n",
    "            if file != 'Summary.txt' and file[-4:] == '.txt':\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file_path[-8:] == 'spam.txt':\n",
    "#                     spam_paths.append(file_path)\n",
    "                    spam_paths = np.append(spam_paths, file_path)\n",
    "                elif file_path[-7:] == 'ham.txt':\n",
    "#                     ham_paths.append(file_path)\n",
    "                    ham_paths = np.append(ham_paths, file_path)\n",
    "                    \n",
    "    dataset = [(path,1) for path in spam_paths] + [(path,0) for path in ham_paths]\n",
    "    random.shuffle(dataset)\n",
    "    \n",
    "    train_ratio = 0.8\n",
    "    validation_ratio = 0.1\n",
    "    num_train = int(train_ratio * len(dataset))\n",
    "    num_validation = int(validation_ratio * len(dataset))\n",
    "    \n",
    "    train_set = dataset[:num_train]\n",
    "    validation_set = dataset[num_train : num_train + num_validation]\n",
    "    test_set = dataset[num_train + num_validation:]\n",
    "    \n",
    "    train_X, train_y = zip(*train_set)\n",
    "    validation_X, validation_y = zip(*validation_set)\n",
    "    test_X, test_y = zip(*test_set)\n",
    "    \n",
    "    \n",
    "    print(f'Number of files: {len(spam_paths)+len(ham_paths)}')\n",
    "    print(f'Number of spam: {len(spam_paths)}')\n",
    "    print(f'Number of ham: {len(ham_paths)}')\n",
    "    print(f\"Size of train_set: {len(train_set)}\")\n",
    "    print(f\"Size of validation_set: {len(validation_set)}\")\n",
    "    print(f\"Size of test_set: {len(test_set)}\")\n",
    "    \n",
    "    return train_X, train_y, validation_X, validation_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_dataset(dataset_X):\n",
    "    \n",
    "    for filePath in dataset_X:\n",
    "        file = open(filePath, 'r', errors='ignore')\n",
    "        yield file\n",
    "        file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfs(file):\n",
    "    result = {}\n",
    "    num_words = 0\n",
    "#     print(file)\n",
    "    for line in file:\n",
    "#         print(line)\n",
    "        words = tokenizer.tokenize(line)\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        for word in words:\n",
    "            if word in result.keys():\n",
    "                result[word] += 1\n",
    "            else:\n",
    "                result[word] = 1\n",
    "                \n",
    "            num_words += 1\n",
    "            \n",
    "    for key in result:\n",
    "        result[key] = result[key] / num_words\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_stats(d):\n",
    "    \"\"\"\n",
    "    Return sorted dictionary by value.\n",
    "    \"\"\"\n",
    "    result = {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idfs(corpus):\n",
    "    \"\"\"\n",
    "    Calculates the idf for every term in the document\n",
    "    \"\"\"\n",
    "    DF = {} #dictionary --> {word: set() of documents in which word present} key:word, value: set\n",
    "    num_documents = len(corpus)\n",
    "    for file in iter_dataset(corpus):\n",
    "#         print(file)\n",
    "        for line in file:\n",
    "#             print(line)\n",
    "            words = tokenizer.tokenize(line)\n",
    "            words = [stemmer.stem(word) for word in words]\n",
    "            for word in words:\n",
    "                try:\n",
    "                    DF[word].add(file)\n",
    "                except:\n",
    "                    DF[word] = {file}\n",
    "    \n",
    "    for key in DF.keys():\n",
    "#         print(len(DF[key]))\n",
    "        DF[key] = np.log(num_documents/(len(DF[key])+1))\n",
    "    \n",
    "    return DF\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(file, idfs):\n",
    "    tfs = get_tfs(file)\n",
    "    tfidfs = {}\n",
    "    for key in tfs:\n",
    "        tfidfs[key] = tfs[key] * idfs[key]\n",
    "        \n",
    "    return tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(idfs):\n",
    "    for file in iter_dataset(train_X[:3]):\n",
    "        tfidfs = get_tfidf(file, idfs)\n",
    "        top20Words = [*tfidfs.keys()][-20:]\n",
    "#         print(dict_stats(top20Words))\n",
    "        print(top20Words)\n",
    "        input()\n",
    "        #pass it to the logistic regression unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 33716\n",
      "Number of spam: 17171\n",
      "Number of ham: 16545\n",
      "Size of train_set: 26972\n",
      "Size of validation_set: 3371\n",
      "Size of test_set: 3373\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, validation_X, validation_y, test_X, test_y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = get_idfs(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject', 'news', 'articl', 'on', 'enron', 'india', 'aftermath', 'busi', 'line', '02', '21', '2000', 'abhay', 'mehta', 'copyright', 'c', 'kasturi', 'sourc', 'world', 'report']\n",
      "\n",
      "['subject', 're', 'industri', 'robert', 'pleas', 'make', 'sure', 'that', 'gari', 'get', 'a', 'copi', 'of', 'the', 'spreadsheet', 'nobodi', 'ha', 'seen', 'it', 'in']\n",
      "\n",
      "['subject', 'largest', 'pornstar', 'collect', 'of', 'download', 'porn', 'd', 'movl', 'x', '881', 'cum', 'wit', 'the', 'most', 'extrem', 'sexual', 'achiev', 'ever', 'to']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\" Returns sigmoid of z \"\"\"\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\" Initialize w array with dim*1 zeros and b as 0\"\"\"\n",
    "    w = np.zeros(dim)\n",
    "    b = 0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, y):\n",
    "    \"\"\"\n",
    "    Apply forward and backward propagation.\n",
    "    \n",
    "    @param w: weights \n",
    "    @precond: np array shape n*1\n",
    "    \n",
    "    @param b: bias \n",
    "    @precond: float\n",
    "    \n",
    "    @param X: the features \n",
    "    @precond: np array shape n*m n is the number of features and m the number of training examples\n",
    "    \n",
    "    @param y: weights \n",
    "    @precond: np array shape 1*m the true values corresponding to X i.e. X(:, i) --> y(i)\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    #Forward prop\n",
    "    z = np.dot(np.transpose(w), X) + b # z: [1*m] matrix\n",
    "    y_hat = sigmoid(z) #y_hat: [1*m] matrix\n",
    "    cost = (-1/m) * np.sum(y * np.log(y_hat) + (1-y) * np.log(1-y_hat)) #number\n",
    "    cost = np.squeeze(cost)\n",
    "    #Backward prop\n",
    "    dw = (1/m) * np.dot(X,np.transpose(y_hat - y))\n",
    "    db = (1/m) * np.sum(y_hat - y)\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, y, epochs, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    Optimizes w, b using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    @param w: weights\n",
    "    @precond: np array shape n * 1\n",
    "\n",
    "    @param b: bias \n",
    "    @precond: -- float\n",
    "    \n",
    "    @param X - the features\n",
    "    @precond: np array shape (n, m)\n",
    "    \n",
    "    @param y - the true labels\n",
    "    @precond: np array (1, m) containing 1 if spam, 0 if ham.\n",
    "\n",
    "    @param epochs\n",
    "    @precond: integer >= 1\n",
    "    \n",
    "    @param learning_rate - lr for gradient descent\n",
    "    @precond: integer > 0\n",
    "    \n",
    "    \n",
    "    Return:\n",
    "    params: dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing dw and db (the gradients)\n",
    "    costs -- list of costs produced every 100 iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w, b, X, y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (â‰ˆ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
